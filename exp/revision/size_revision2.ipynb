{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominant-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hdbscan\n",
    "import warnings\n",
    "import sklearn.cluster\n",
    "import scipy.cluster\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy.linalg import norm\n",
    "from classix.aggregation_test import aggregate\n",
    "from classix import CLASSIX\n",
    "from quickshift.QuickshiftPP import *\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from threadpoolctl import threadpool_limits\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def benchmark_algorithm_tsize(dataset_sizes, cluster_function, function_args, function_kwds,\n",
    "                        dataset_dimension=10, dataset_n_clusters=20, max_time=45, sample_size=10, algorithm=None):\n",
    "\n",
    "    # Initialize the result with NaNs so that any unfilled entries\n",
    "    # will be considered NULL when we convert to a pandas dataframe at the end\n",
    "    result_time = np.nan * np.ones((len(dataset_sizes), sample_size))\n",
    "    result_ar = np.nan * np.ones((len(dataset_sizes), sample_size))\n",
    "    result_ami = np.nan * np.ones((len(dataset_sizes), sample_size))\n",
    "    \n",
    "    for index, size in enumerate(dataset_sizes):\n",
    "        for s in range(sample_size):\n",
    "            # Use sklearns make_blobs to generate a random dataset with specified size\n",
    "            # dimension and number of clusters\n",
    "            # set cluster_std=0.1 to ensure clustering rely less on tuning parameters.\n",
    "            data, labels = sklearn.datasets.make_blobs(n_samples=size,\n",
    "                                                       n_features=dataset_dimension,\n",
    "                                                       centers=dataset_n_clusters, \n",
    "                                                       cluster_std=1) \n",
    "\n",
    "            # Start the clustering with a timer\n",
    "            start_time = time.time()\n",
    "            cluster_function.fit(data, *function_args, **function_kwds)\n",
    "            time_taken = time.time() - start_time\n",
    "            if algorithm == \"Quickshift++\":\n",
    "                preds = cluster_function.memberships\n",
    "            else:\n",
    "                preds = cluster_function.labels_\n",
    "            # print(\"labels num:\", len(np.unique(preds))) \n",
    "            ar = metrics.adjusted_rand_score(labels, preds)\n",
    "            ami = metrics.adjusted_mutual_info_score(labels, preds)\n",
    "            # If we are taking more than max_time then abort -- we don't\n",
    "            # want to spend excessive time on slow algorithms\n",
    "            if time_taken > max_time: # Luckily, it won't happens in our experiment.\n",
    "                result_time[index, s] = time_taken\n",
    "                result_ar[index, s] = ar\n",
    "                result_ami[index, s] = ami\n",
    "                return pd.DataFrame(np.vstack([dataset_sizes.repeat(sample_size), result_time.flatten()]).T, columns=['x','y']), \\\n",
    "                       pd.DataFrame(np.vstack([dataset_sizes.repeat(sample_size), result_ar.flatten()]).T, columns=['x','y']), \\\n",
    "                       pd.DataFrame(np.vstack([dataset_sizes.repeat(sample_size), result_ami.flatten()]).T, columns=['x','y'])\n",
    "            else:\n",
    "                result_time[index, s] = time_taken\n",
    "                result_ar[index, s] = ar\n",
    "                result_ami[index, s] = ami\n",
    "\n",
    "    # Return the result as a dataframe for easier handling with seaborn afterwards\n",
    "    return pd.DataFrame(np.vstack([dataset_sizes.repeat(sample_size), result_time.flatten()]).T, columns=['x','y']), \\\n",
    "           pd.DataFrame(np.vstack([dataset_sizes.repeat(sample_size), result_ar.flatten()]).T, columns=['x','y']), \\\n",
    "           pd.DataFrame(np.vstack([dataset_sizes.repeat(sample_size), result_ami.flatten()]).T, columns=['x','y'])\n",
    "\n",
    "\n",
    "def rn_gaussian_size():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    sns.set_context('poster')\n",
    "    sns.set_palette('Paired', 10)\n",
    "    sns.set_color_codes()\n",
    "    np.random.seed(0)\n",
    "    dataset_sizes = np.hstack([np.arange(1, 11) * 5000])\n",
    "\n",
    "    np.random.seed(0)\n",
    "    with threadpool_limits(limits=1, user_api='blas'):\n",
    "        k_means = sklearn.cluster.KMeans(n_clusters=20, init='k-means++')\n",
    "        k_means_time, k_means_ar, k_means_ami = benchmark_algorithm_tsize(dataset_sizes, k_means, (), {})\n",
    "\n",
    "        dbscan = sklearn.cluster.DBSCAN(eps=3, min_samples=1, n_jobs=1, algorithm='ball_tree')\n",
    "        dbscan_btree_time, dbscan_btree_ar, dbscan_btree_ami = benchmark_algorithm_tsize(dataset_sizes, dbscan, (), {})\n",
    "\n",
    "        dbscan = sklearn.cluster.DBSCAN(eps=3, min_samples=1, n_jobs=1, algorithm='kd_tree')\n",
    "        dbscan_kdtree_time, dbscan_kdtree_ar, dbscan_kdtree_ami = benchmark_algorithm_tsize(dataset_sizes, dbscan, (), {})\n",
    "\n",
    "        hdbscan_ = hdbscan.HDBSCAN(algorithm='best', core_dist_n_jobs=1)\n",
    "        hdbscan_time, hdbscan_ar, hdbscan_ami = benchmark_algorithm_tsize(dataset_sizes, hdbscan_, (), {})\n",
    "\n",
    "        classix = CLASSIX(sorting='pca', radius=0.3, minPts=5, group_merging='distance', verbose=0) \n",
    "        classix_time, classix_ar, classix_ami = benchmark_algorithm_tsize(dataset_sizes, classix, (), {})\n",
    "        \n",
    "        quicks = QuickshiftPP(k=20, beta=0.7)\n",
    "        quicks_time, quicks_ar, quicks_ami = benchmark_algorithm_tsize(dataset_sizes, quicks, (), {}, algorithm='Quickshift++')\n",
    "\n",
    "\n",
    "    k_means_time.to_csv(\"results/exp1/gs_kmeans_time2.csv\",index=False)\n",
    "    dbscan_kdtree_time.to_csv(\"results/exp1/gs_dbscan_kdtree_time2.csv\",index=False)\n",
    "    dbscan_btree_time.to_csv(\"results/exp1/gs_dbscan_btree_time2.csv\",index=False)\n",
    "    hdbscan_time.to_csv(\"results/exp1/gs_hdbscan_time2.csv\",index=False)\n",
    "    classix_time.to_csv(\"results/exp1/gs_classix_time2.csv\",index=False)\n",
    "    quicks_time.to_csv(\"results/exp1/gs_quicks_time2.csv\",index=False)\n",
    "    \n",
    "    k_means_ar.to_csv(\"results/exp1/gs_kmeans_ar2.csv\",index=False)\n",
    "    dbscan_kdtree_ar.to_csv(\"results/exp1/gs_dbscan_kdtree_ar2.csv\",index=False)\n",
    "    dbscan_btree_ar.to_csv(\"results/exp1/gs_dbscan_btree_ar2.csv\",index=False)\n",
    "    hdbscan_ar.to_csv(\"results/exp1/gs_hdbscan_ar2.csv\",index=False)\n",
    "    classix_ar.to_csv(\"results/exp1/gs_classix_ar2.csv\",index=False)\n",
    "    quicks_ar.to_csv(\"results/exp1/gs_quicks_ar2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uniform-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_gaussian_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-advancement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
